# Stability of large ecological communities

```{r, include=FALSE}
source("common_settings.R")
```
**Lesson plan**:

1. We introduce the so-called diversity-stability debate, by discussing the seminal contributions of MacArthur (1955) and May (1972).
1. We show how the celebrated May's stability criterion can be derived using elementary results from random matrix theory.
1. We extend May's results to the case in which interactions between the species are correlated.
1. We discuss a few tools from random matrix theory that are useful for ecology.

## Complexity and stability

:::boxpaper

**Key paper: @macarthur1955fluctuations** 

In 1955, MacArthur used an information-theoretic argument to suggest more speciose communities would be more "stable": having a food web with many species would open several "channels" for energy to flow from resources to consumers, such that if a channel was to go dry, another channel could take its place.

**Key paper: @may1972will**

This idea was challenged by May, who showed that, all other things being equal, larger communities would be less likely to be *dynamically stable*. This paper set the so-called "diversity-stability" debate that still populates the literature.

**Key paper: @mccann2000diversity** 

In this review, McCann summarizes the development of the diversity-stability debate over the span of three decades.

:::

As we have seen before, an equilibrium $x^\star$ is stable if the community matrix for the equilibrium has all eigenvalues with negative real part. In general, to determine the equilibrium and its stability, we would need to specify all the growth rates ($r$, $n$ values), as well as the matrix of interactions ($A$, $n^2$ values). This is impractical to do for large systems (though we will try this out later). But can something quite general be said about the limit in which many species are in the community?


## The stability of random ecosystems

:::boxhistory

**History: Robert M. May (1936-2020)**

```{r, echo=FALSE,out.width = "25%"}
knitr::include_graphics('img/May.jpg')
```

Born and educated in Sidney, May received a PhD in theoretical physics from the University of Sidney in 1959. Having started his career in the physics of super-conductivity, May became interested in ecology. When visiting Princeton during his stay at the Institute for Advanced Studies, he met MacArthur, and decided to change fields. 

He ended up changing the field of ecology, with an incredible number of contributions on population dynamics and chaos (see key papers in preceding lectures), the dynamics of infectious diseases (with Roy Anderson, @anderson1992infectious), evolutionary game theory (@nowak1992evolutionary), the estimation of the number of species on Earth (@may1988many), bibliometrics (@may1997scientific), and even banking (by modeling systemic risks, @haldane2011systemic). He succeeded MacArthur at Princeton, and concluded his career in England (University of Oxford). 

He won a large number of prizes, and was created a life peer (Lord May, Baron of Oxford). He served as president of the Royal Society and as Chief Scientific Adviser to the government.

:::

@may1972will attempted to answer this question by considering a **random community matrix**. In a GLV system, the diagonal elements $M_{ii} = A_{ii} x_i^\star$ are influenced by self-regulation (i.e., as in a carrying capacity), while the off-diagonal elements $M_{ij} = A_{ij} x_i^\star$ model the effect of species $j$ on the equilibrium of species $i$. May considered the following algorithm to build a random community matrix. Take a large community, resting at an unspecified, feasible equilibrium; we build the community matrix by setting:

- $M_{ij} = 0$  with probability $(1-C)$; with probability $C$ we draw $M_{ij}$ from a distribution with mean zero and variance $\sigma^2$. $C$ is the proportion of realized connections, termed the "connectance" of the system.
- the diagonal elements are set to $-d$, modeling self-regulation.

```{r, buildmay, echo=FALSE}
build_May_normal <- function(n, C, d, sigma){
  # fill the whole matrix
  M <- matrix(rnorm(n * n, mean = 0, sd = sigma), n, n)
  # remove connections 
  M <- M * matrix(runif(n * n) <= C, n, n)
  # set diagonals
  diag(M) <- -d
  return(M)
}
```

May did not specify the distribution from which one would draw the nonzero interactions (more on this later). For the moment, let's assume it's a Normal distribution with mean zero and variance $\sigma^2$.

Note that the average of the eigenvalues of a matrix $A$ is given by the average of its diagonal elements $\frac{1}{n}\sum_i \lambda_i = \frac{1}{n} \text{Tr}(A) = \frac{1}{n} \sum_i A_{ii}$. As such, if $A = dI + B$, the eigenvalues of $A$ will be those of $B$ shifted by $d$.

We want to determine whether the equilibrium will be stable, given $n$, $C$, $d$ and $\sigma^2$. To do so, we need to find the location of the "rightmost" eigenvalue of $M$. For example, let's plot the eigenvalues of a large matrix ($500 \times 500$, the red-dashed line marks the location of zero on the x axis):

```{r, exmay, echo = FALSE}
plot_eigenvalues <- function(M, prediction = NULL){
  eig <- eigen(M, only.values = TRUE)$values
  dt <- tibble(Real = Re(eig), Imaginary = Im(eig))
  pl <- ggplot(dt) + aes(x = Real, y = Imaginary) + 
    geom_point() + 
    coord_equal() + 
    geom_vline(xintercept = 0, colour = "red", linetype = 2)
  if (is.null(prediction) == FALSE) {
    pl <- pl + geom_vline(xintercept = prediction, colour = "black", linetype = 2) + my_theme
  }
  show(pl)
}
set.seed(100) # for reproducibility
# parameters
n <- 500
C <- 0.5
d <- 10
sigma <- 1
M <- build_May_normal(n, C, d, sigma)
plot_eigenvalues(M)
```

The eigenvalues fall into an almost perfect circle! Turns out, that this is the behavior we should expect, as stated by the so-called "Circular Law", one of the most beautiful results in random matrix theory. 

**Circular law:** Take a non-symmetric, $n \times n$ random matrix in which all coefficients $X_{ij}$ are i.i.d. random variables with $\mathbb E[X_{ij}] = 0$ and $\mathbb E[X_{ij}] = 1$. Then, as $n \to \infty$, the e.s.d. of ${X} / \sqrt{n}$ converges to the circular law:

$$
  \mu(\lambda) = \begin{cases}
    \frac{1}{\pi} \; \; \; \text{if} \; (\text{Re}(\lambda))^2 +
    (\text{Im}(\lambda))^2 \leq 1\\
    0 \; \; \;\text{ otherwise}.
  \end{cases}
$$

This result can be used to calculate the radius of the eigenvalue distribution of the matrices studied by May: when the off-diagonal coefficients $M_{ij}$ are 0 with probability $1-C$ and are sampled independently from a distribution with mean $0$ and variance $\sigma^2$ with probability $C$, we have that $\mathbb E[M_{ij}] = 0$ and $\mathbb E[M_{ij}^2] = C \sigma^2$. This means that if we were to divide the coefficients of ${M}$ by $\sqrt{C \sigma^2}$ we would recover the unit variance, and the matrix would follow the circular law when $S$ is large. Armed with this, we can calculate the radius: if the radius of ${M} / \sqrt{n C \sigma^2}$ converges to 1 when the matrix is large, then the radius of ${M}$ is approximately $\sqrt{n C \sigma^2}$. For stability, we need a sufficiently negative diagonal (setting the center of the circle), yielding May's stability criterion:

$$
\sqrt{n C \sigma^2} < d
$$

We can try this on our matrix (black dashed line):

```{r, maycrit, echo = FALSE}
prediction <- sqrt(n * C * sigma^2) - d
plot_eigenvalues(M, prediction)
```

Showing that we accurately approximate the location of the rightmost eigenvalue. Note that, in the case of large $n$, whenever the circle crosses zero, some eigenvalues will be positive, determining the instability of the equilibrium.

Importantly, the distribution from which the coefficients are sampled does not matter---only that the mean is zero and that the variance is $\sigma^2$. For example, build the matrix using coefficients from a uniform distribution:

```{r, exmay2,echo=FALSE}
build_May_uniform <- function(n, C, d, sigma){
  # fill the whole matrix (sqrt(3) to ensure var = sigma^2)
  M <- matrix(runif(n * n, min = -sqrt(3) * sigma, max = sqrt(3) * sigma), n, n)
  # remove connections 
  M <- M * matrix(runif(n * n) <= C, n, n)
  # set diagonals
  diag(M) <- -d
  return(M)
}
# parameters
n <- 500
C <- 0.5
d <- 10
sigma <- 1
M <- build_May_uniform(n, C, d, sigma)
prediction <- sqrt(n * C * sigma^2) - d
plot_eigenvalues(M, prediction)
```

This property is called **universality** in random matrix theory.

:::boxhomework

**Homework 4a**

The probability that a matrix is stable, given $C$, $\sigma$ and $n$ is close to 1 when the stability criterion is satisfied, and close to 0 when it is not. Matrices satisfying $\sqrt{n C \sigma^2} = d$ are the critical point. In theory, the results only hold in the limit $n \to \infty$ (to be accurate, $nC \to \infty$), as eigenvalues can fall outside the circle with small probability. 

* Write code to compute the real part for the "rightmost" eigenvalue of a random matrix (Note: computing eigenvalues is fairly expensive in terms of computing time. Use `eigen(M, only.values = TRUE)$values` to speed up calculations).
* Write code to build matrices like those studied by May (nonzero elements sampled from a normal distribution).
* Set $d = 10$ and choose parameters $n$, $C$ and $\sigma^2$ such that you are close to the critical point (make sure $n$ and $C$ are large enough, for example $nC > 10$). Draw 1000 random matrices and compute the probability drawing a stable matrix.
* Vary $n$, $C$ and $\sigma^2$ in turn, making them cross the critical point. Draw a graph where the probability of stability is on the y axis, the x axis measures $\sqrt{nC\sigma^2}$. The graph should look like the one reported below:

```{r, exercisemay, message=FALSE, warning=FALSE,echo=FALSE}
library(latex2exp)
dt <- read.csv("dat/MayExercise.csv")
pl_p <- ggplot(dt) + 
  aes(x = complexity, y = probability, shape = change) + 
  geom_point(size = 2) + 
  geom_vline(xintercept = 10, linetype = 2) + 
  scale_shape_manual(name = "Varying:", labels = expression(C[phantom(2)] ~~ 0.1 %->% 0.3,
                                                            sigma^2 ~~ 0.5 %->% 1.5, 
                                                            n ~~250 %->% 750),  
                     values = c(1, 4, 3)) + 
  xlab(label = latex2exp::TeX('$\\sqrt{n C \\sigma^2}$')) + 
  ylab("Probability of stability") + my_theme + scale_colour_manual(values = my_colors_7)
pl_p
```

:::

## Accounting for interaction types

In ecological communities, the effect of species $i$ on $j$ and that of $j$ on $i$ are typically not independent (as assumed above): in the case of competition between species, we expect them both to be negative; for consumption, if one is positive, the other is negative, and so forth. A more refined model of a random matrix would therefore sample interactions in pairs from a bivariate distribution. The elliptic law can deal with this case:

**Elliptic law:** Take a non-symmetric, $n \times n$ random matrix in which the pairs of coefficients $(X_{ij}, X_{ji})$ are sampled independently from a bivariate distribution defined by a vector of means $m = (0,0)^t$ and a covariance matrix $\Sigma = \begin{pmatrix} 1 & \rho\\ \rho & 1 \end{pmatrix}$. Then, as $n \to \infty$, the e.s.d. of ${n} / \sqrt{S}$ converges to the elliptic law:

$$
  \mu(\lambda) = \begin{cases} \frac{1}{\pi (1 - \rho^2) } \quad
    \text{if} \; \frac{(\text{Re}(\lambda))^2}{(1 + \rho)^2} +
    \frac{(\text{Im}(\lambda))^2}{(1 -\rho)^2} \leq
    1\\ 0 \quad \quad \quad \text{ otherwise}
  \end{cases}
$$

Note that when $\rho = 0$, the elliptic law reduces to the circular law. Using the elliptic law, @allesina2012stability were able to extend May's criterion to ecological networks with different mixtures of interaction types. 

Build a matrix $M$ by sampling the entries in pairs: $(M_{ij}, M_{ji})$ are zero with probability $(1-C)$, and with probability $C$ sampled independently from a bivariate distribution with mean $\nu = (0, 0)^T$, and covariance matrix $\Sigma = \sigma^2 \begin{pmatrix} 1 & \rho\\ \rho & 1\end{pmatrix}$. Then $\mathbb E[M_{ij}] = 0$, $\mathbb E[M_{ij}^2] = C \sigma^2$, and $E[M_{ij} M_{ji}] = C \sigma^2 \rho$. By dividing the entries ${M}$ by $\sqrt{C \sigma^2}$, we obtain a matrix following the elliptic law. As such, the stability criterion becomes:

$$
\sqrt{n C \sigma^2}(1 + \rho) < d
$$

To see the elliptic law in action, we can build matrices in which we sample the coefficients in pairs from a bivariate normal distribution. If we sample the entries from a distribution with a positive correlation, we obtain a horizontally-stretched ellipse (and hence, more difficult to stabilize than the circle):

```{r, allesinatangbuild,echo= FALSE}
build_Allesina_Tang_normal <- function(n, C, d, sigma, rho){
  # sample coefficients in pairs
  pairs <- MASS::mvrnorm(n = n * (n-1) / 2,
                         mu = c(0, 0),
                         Sigma = sigma^2 * matrix(c(1, rho, rho, 1), 2, 2))
  # build a completely filled matrix
  M <- matrix(0, n, n)
  M[upper.tri(M)] <- pairs[,1]
  M <- t(M)
  M[upper.tri(M)] <- pairs[,2]
  # determine which connections to retain
  Connections <- (matrix(runif(n * n), n, n) <= C) * 1 
  Connections[lower.tri(Connections)] <- 0
  diag(Connections) <- 0
  Connections <- Connections + t(Connections)
  M <- M * Connections
  diag(M) <- -d
  return(M)
}
```

```{r, allesinatangex1, echo = FALSE}
# parameters
n <- 500
C <- 0.5
d <- 10
sigma <- 1
rho <- 0.4
M <- build_Allesina_Tang_normal(n, C, d, sigma, rho)
prediction <- sqrt(n * C * sigma^2) * (1 + rho) - d
plot_eigenvalues(M, prediction)
```

Similarly, a negative correlation (e.g., as in predator-prey) would make the system easier to stabilize:

```{r, allesinatangex2, echo = FALSE}
# parameters
n <- 500
C <- 0.5
d <- 10
sigma <- 1
rho <- -0.4
M <- build_Allesina_Tang_normal(n, C, d, sigma, rho)
prediction <- sqrt(n * C * sigma^2) * (1 + rho) - d
plot_eigenvalues(M, prediction)
```

Allesina and Tang therefore concluded that, all other things being equal, ecological communities in which predator-prey interactions are prevalent (and as such $E[M_{ij} M_{ji}] < 0$) are easier to stabilize than those dominated by competition/mutualism ($E[M_{ij} M_{ji}] > 0$).

## Symmetric matrices

Build a random symmetric matrix $X$ by sampling the coefficients $X_{ij}$ in the upper-triangular part (i.e., $i < j$) independently from a distribution such that $\mathbb E[X_{ij}] = 0$ and $\mathbb E[X_{ij}] = 1$. Set $X_{ji} = X_{ij}$, thereby building a symmetric matrix. The diagonal elements are sampled from a distribution with mean zero and finite variance. Then, as $n \to \infty$, the empirical spectral distribution of $X / \sqrt{n}$ (i.e., of the matrix in which all the coefficients have been divided by the $\sqrt{n}$) converges almost surely to the Wigner's semicirle distribution:

\begin{equation}
  \mu(\lambda) = \begin{cases} \frac{1}{2 \pi} \sqrt{4 - \lambda^2}\;
    \; \; \text{if}\; \lambda \in [-2, 2]\\
    0 \; \; \;\text{ otherwise}.
  \end{cases}
\end{equation}

Importantly, we have not defined the distribution of the $X_{ij}$: as long as the coefficients have mean zero, and unit variance, the result holds (universality).

```{r, wigner, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(3)
n <- 1000
A <- matrix(rnorm(n * n), n, n)
A[lower.tri(A)] <- 0
A <- A + t(A)
A <- A / sqrt(n)
diag(A) <- 0

eA <- eigen(A, only.values = TRUE, symmetric = TRUE)$values
deA <- data.frame(eigenvalue = eA)

pl <- ggplot(deA) + aes(x = eigenvalue) + 
  geom_histogram(aes(y = stat(density)), binwidth = 0.1) + 
  geom_hline(yintercept = 0) 

dwigner <- function(x, center, n, var){
  x <- (x - center) / sqrt(n)
  return((1 / (2 * pi * var)) * sqrt(4 * var - x^2) / sqrt(n))
}

to_plot <- seq(min(eA) - 0.1, max(eA) + 0.1, by = 0.01)
df_dens <- data.frame(eigenvalue = to_plot, density = dwigner(to_plot, center = 0, n = 1, var = 1))
df_dens[is.na(df_dens)] <- 0

pl <- pl + geom_path(data = df_dens, aes(x = eigenvalue, y = density)) + my_theme + 
  xlab(label = latex2exp::TeX('$\\lambda_i$'))
pl 
```

## Covariance matrices

Take a $p \times n$ rectangular matrix $X$, with $p < n$ and i.i.d. coefficients with $\mathbb E[X_{ij}] = 0$ and $\mathbb E[X_{ij}] = 1$. When $n \to \infty$, the ratio $p/n \to y$ (i.e., the number of rows and columns grow proportionally). Then the eigenvalue distribution of the scaled covariance matrix $S = \frac{1}{n} X X^T$ converges to the Marchenko-Pastur distribution:

$$
\begin{equation}
  \mu(\lambda) = \begin{cases} \frac{1}{2 \pi \lambda y}
    \sqrt{\left((1 + \sqrt{y})^2 - \lambda \right) \left(\lambda - (1 -
      \sqrt{y})^2 \right)}\; \; \; \text{if} \; (1 - \sqrt{y})^2 \leq
    \lambda \leq (1 + \sqrt{y})^2\\ 0 \; \; \;\text{ otherwise}.
  \end{cases}
\end{equation}
$$

## Small-rank perturbations



## Further readings

On random matrices and stability:

- @allesina2015stability is an opinionated review on applications of random matrices in ecology.





